{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import MEArec as mr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet as cc\n",
    "import math\n",
    "import os\n",
    "\n",
    "def get_ccolor(k):\n",
    "    if k == -1:\n",
    "        return \"#808080\"\n",
    "    else:\n",
    "        return ccolors[k % len(ccolors)]\n",
    "ccolors = cc.glasbey[:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732eee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from brainbox.io.one import SpikeSortingLoader\n",
    "from ibllib.atlas import AllenAtlas\n",
    "from one.api import ONE\n",
    "import brainbox.io.one as bbone\n",
    "import numpy as np\n",
    "import datetime\n",
    "from spike_psvae.subtract import read_geom_from_meta\n",
    "from pathlib import Path\n",
    "from spike_psvae.waveform_utils import make_channel_index, make_contiguous_channel_index\n",
    "import matplotlib.pyplot as plt\n",
    "from spike_psvae import denoise, snr_templates, spike_train_utils\n",
    "import torch\n",
    "from spike_psvae.spikeio import read_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def kill_signal(recordings, threshold, window_size):\n",
    "    \"\"\"\n",
    "    Thresholds recordings, values above 'threshold' are considered signal\n",
    "    (set to 0), a window of size 'window_size' is drawn around the signal\n",
    "    points and those observations are also killed\n",
    "    Returns\n",
    "    -------\n",
    "    recordings: numpy.ndarray\n",
    "        The modified recordings with values above the threshold set to 0\n",
    "    is_noise_idx: numpy.ndarray\n",
    "        A boolean array with the same shap as 'recordings' indicating if the\n",
    "        observation is noise (1) or was killed (0).\n",
    "    \"\"\"\n",
    "    recordings = np.copy(recordings)\n",
    "\n",
    "    T, C = recordings.shape\n",
    "    R = int((window_size-1)/2)\n",
    "\n",
    "    # this will hold a flag 1 (noise), 0 (signal) for every obseration in the\n",
    "    # recordings\n",
    "    is_noise_idx = np.zeros((T, C))\n",
    "\n",
    "    # go through every neighboring channel\n",
    "    for c in range(C):\n",
    "\n",
    "        # get obserations where observation is above threshold\n",
    "        idx_temp = np.where(np.abs(recordings[:, c]) > threshold)[0]\n",
    "\n",
    "        if len(idx_temp) == 0:\n",
    "            is_noise_idx[:, c] = 1\n",
    "            continue\n",
    "\n",
    "        # shift every index found\n",
    "        for j in range(-R, R+1):\n",
    "\n",
    "            # shift\n",
    "            idx_temp2 = idx_temp + j\n",
    "\n",
    "            # remove indexes outside range [0, T]\n",
    "            idx_temp2 = idx_temp2[np.logical_and(idx_temp2 >= 0,\n",
    "                                                 idx_temp2 < T)]\n",
    "\n",
    "            # set surviving indexes to nan\n",
    "            recordings[idx_temp2, c] = np.nan\n",
    "\n",
    "        # noise indexes are the ones that are not nan\n",
    "        # FIXME: compare to np.nan instead\n",
    "        is_noise_idx_temp = (recordings[:, c] == recordings[:, c])\n",
    "\n",
    "        # standarize data, ignoring nans\n",
    "        recordings[:, c] = recordings[:, c]/np.nanstd(recordings[:, c])\n",
    "\n",
    "        # set non noise indexes to 0 in the recordings\n",
    "        recordings[~is_noise_idx_temp, c] = 0\n",
    "\n",
    "        # save noise indexes\n",
    "        is_noise_idx[is_noise_idx_temp, c] = 1\n",
    "\n",
    "    return recordings, is_noise_idx\n",
    "\n",
    "\n",
    "def noise_whitener(recordings, temporal_size, window_size, sample_size=1000,\n",
    "                   threshold=3.0, max_trials_per_sample=10000,\n",
    "                   allow_smaller_sample_size=False):\n",
    "    \"\"\"Compute noise temporal and spatial covariance\n",
    "    Parameters\n",
    "    ----------\n",
    "    recordings: numpy.ndarray\n",
    "        Recordings\n",
    "    temporal_size:\n",
    "        Waveform size\n",
    "    sample_size: int\n",
    "        Number of noise snippets of temporal_size to search\n",
    "    threshold: float\n",
    "        Observations below this number are considered noise\n",
    "    Returns\n",
    "    -------\n",
    "    spatial_SIG: numpy.ndarray\n",
    "    temporal_SIG: numpy.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    # kill signal above threshold in recordings\n",
    "    print('Get Noise Floor')\n",
    "    rec, is_noise_idx = kill_signal(recordings, threshold, window_size)\n",
    "\n",
    "    # compute spatial covariance, output: (n_channels, n_channels)\n",
    "    print('Compute Spatial Covariance')\n",
    "    spatial_cov = np.divide(np.matmul(rec.T, rec),\n",
    "                            np.matmul(is_noise_idx.T, is_noise_idx))\n",
    "    spatial_cov[np.isnan(spatial_cov)] = 0\n",
    "    spatial_cov[np.isinf(spatial_cov)] = 0\n",
    "\n",
    "    # compute spatial sig\n",
    "    w_spatial, v_spatial = np.linalg.eig(spatial_cov)\n",
    "    spatial_SIG = np.matmul(np.matmul(v_spatial,\n",
    "                                      np.diag(np.sqrt(w_spatial))),\n",
    "                            v_spatial.T)\n",
    "\n",
    "    # apply spatial whitening to recordings\n",
    "    print('Compute Temporal Covaraince')\n",
    "    spatial_whitener = np.matmul(np.matmul(v_spatial,\n",
    "                                           np.diag(1/np.sqrt(w_spatial))),\n",
    "                                 v_spatial.T)\n",
    "    #print (\"rec: \", rec, \", spatial_whitener: \", spatial_whitener.shape)\n",
    "    rec = np.matmul(rec, spatial_whitener)\n",
    "\n",
    "    # search single noise channel snippets\n",
    "    noise_wf = search_noise_snippets(\n",
    "        rec, is_noise_idx, sample_size,\n",
    "        temporal_size,\n",
    "        channel_choices=None,\n",
    "        max_trials_per_sample=max_trials_per_sample,\n",
    "        allow_smaller_sample_size=allow_smaller_sample_size)\n",
    "\n",
    "    w, v = np.linalg.eig(np.cov(noise_wf.T))\n",
    "\n",
    "    temporal_SIG = np.matmul(np.matmul(v, np.diag(np.sqrt(w))), v.T)\n",
    "\n",
    "    return spatial_SIG, temporal_SIG\n",
    "\n",
    "\n",
    "def search_noise_snippets(recordings, is_noise_idx, sample_size,\n",
    "                          temporal_size, channel_choices=None,\n",
    "                          max_trials_per_sample=100000,\n",
    "                          allow_smaller_sample_size=False):\n",
    "    \"\"\"\n",
    "    Randomly search noise snippets of 'temporal_size'\n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_choices: list\n",
    "        List of sets of channels to select at random on each trial\n",
    "    max_trials_per_sample: int, optional\n",
    "        Maximum random trials per sample\n",
    "    allow_smaller_sample_size: bool, optional\n",
    "        If 'max_trials_per_sample' is reached and this is True, the noise\n",
    "        snippets found up to that time are returned\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if after 'max_trials_per_sample' trials, no noise snippet has been\n",
    "        found this exception is raised\n",
    "    Notes\n",
    "    -----\n",
    "    Channels selected at random using the random module from the standard\n",
    "    library (not using np.random)\n",
    "    \"\"\"\n",
    "\n",
    "    T, C = recordings.shape\n",
    "\n",
    "    if channel_choices is None:\n",
    "        noise_wf = np.zeros((sample_size, temporal_size))\n",
    "    else:\n",
    "        lenghts = set([len(ch) for ch in channel_choices])\n",
    "\n",
    "        if len(lenghts) > 1:\n",
    "            raise ValueError('All elements in channel_choices must have '\n",
    "                             'the same length, got {}'.format(lenghts))\n",
    "\n",
    "        n_channels = len(channel_choices[0])\n",
    "        noise_wf = np.zeros((sample_size, temporal_size, n_channels))\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    print('Starting to search noise snippets...')\n",
    "\n",
    "    trial = 0\n",
    "\n",
    "    # repeat until you get sample_size noise snippets\n",
    "    while count < sample_size:\n",
    "\n",
    "        # random number for the start of the noise snippet\n",
    "        t_start = np.random.randint(T-temporal_size)\n",
    "\n",
    "        if channel_choices is None:\n",
    "            # random channel\n",
    "            ch = random.randint(0, C - 1)\n",
    "        else:\n",
    "            ch = random.choice(channel_choices)\n",
    "\n",
    "        t_slice = slice(t_start, t_start+temporal_size)\n",
    "\n",
    "        # get a snippet from the recordings and the noise flags for the same\n",
    "        # location\n",
    "        snippet = recordings[t_slice, ch]\n",
    "        snipped_idx_noise = is_noise_idx[t_slice, ch]\n",
    "\n",
    "        # check if all observations in snippet are noise\n",
    "        if snipped_idx_noise.all():\n",
    "            # add the snippet and increase count\n",
    "            noise_wf[count] = snippet\n",
    "            count += 1\n",
    "            trial = 0\n",
    "\n",
    "            print('Found %i/%i...', count, sample_size)\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "        if trial == max_trials_per_sample:\n",
    "            if allow_smaller_sample_size:\n",
    "                return noise_wf[:count]\n",
    "            else:\n",
    "                raise ValueError(\"Couldn't find snippet {} of size {} after \"\n",
    "                                 \"{} iterations (only {} found)\"\n",
    "                                 .format(count + 1, temporal_size,\n",
    "                                         max_trials_per_sample,\n",
    "                                         count))\n",
    "\n",
    "    return noise_wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7504c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, num_train, num_val, num_test, n_chans):\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    tot_num = num_train + num_val + num_test\n",
    "    n_div = int(len(data) / tot_num)\n",
    "    for i in range(n_div):\n",
    "        train_set.append(data[num_train*i:num_train*(i+1)])\n",
    "        val_set.append(unit[num_train*(i+1):num_train*(i+1)+num_val])\n",
    "        test_set.append(unit[num_train*(i+1)+num_val:num_train*(i+1)+num_val+num_test])\n",
    "\n",
    "    train_set = np.array(train_set).reshape(-1, n_chans, 2)\n",
    "    val_set = np.array(val_set).reshape(-1, n_chans, 2)\n",
    "    test_set = np.array(test_set).reshape(-1, n_chans, 2)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def pad_channels(wf, geoms, mc_start, mc_end, n_chans):\n",
    "    curr_n_chans = mc_end - mc_start\n",
    "    pad_beg = n_chans - curr_n_chans if mc_start == 0 else 0\n",
    "    pad_end = n_chans - curr_n_chans if mc_start > 0 else 0\n",
    "    \n",
    "    wf_len = wf.shape[0]\n",
    "    if pad_beg + wf_len + pad_end != n_chans:\n",
    "        print(mc_start)\n",
    "        print(mc_end)\n",
    "        print(pad_beg)\n",
    "        print(pad_end)\n",
    "        print('bad wf')\n",
    "    \n",
    "    pad_beg_wf = np.zeros((pad_beg, 121))\n",
    "    pad_end_wf = np.zeros((pad_end, 121))\n",
    "    wf = np.concatenate([pad_beg_wf, wf, pad_end_wf])\n",
    "    \n",
    "    pad_beg_cn = -1 * np.ones((pad_beg,))\n",
    "    pad_end_cn = -1 * np.ones((pad_end,))\n",
    "    chan_nums = np.concatenate([pad_beg_cn, np.arange(mc_start, mc_end), pad_end_cn])\n",
    "    \n",
    "    pad_beg_geom = np.zeros((pad_beg,2))\n",
    "    pad_end_geom = np.zeros((pad_end,2))\n",
    "    geoms = np.concatenate([pad_beg_geom, geoms, pad_end_geom])\n",
    "    \n",
    "    pad_beg_cn = np.zeros((pad_beg,))\n",
    "    pad_end_cn = np.zeros((pad_end,))\n",
    "    mask = np.concatenate([pad_beg_cn, np.ones((curr_n_chans)), pad_end_cn])\n",
    "    \n",
    "    return wf, chan_nums, geoms, mask\n",
    "\n",
    "def save_sim_covs(rec_path, save_path):\n",
    "    recgen = mr.load_recordings(rec_path, load_waveforms=False)\n",
    "    \n",
    "    rec = si.MEArecRecordingExtractor(rec_path)\n",
    "    rec = si.bandpass_filter(rec, dtype='float32')\n",
    "    rec = si.common_reference(rec, reference='global', operator='median')\n",
    "    rec = si.zscore(rec)\n",
    "    \n",
    "    norm_chan_recording = rec.get_traces()\n",
    "    \n",
    "    spatial_cov, temporal_cov = noise_whitener(norm_chan_recording, 121, 50)\n",
    "    \n",
    "    np.save(os.path.join(save_path, '/spatial_cov.npy'), spatial_cov)\n",
    "    np.save(os.path.join(save_path, '/temporal_cov.npy'), temporal_cov)\n",
    "    \n",
    "    \n",
    "# def save_real_covs(rec_path, save_path):\n",
    "#     recgen = mr.load_recordings(rec_path, load_waveforms=False)\n",
    "    \n",
    "#     rec = si.MEArecRecordingExtractor(rec_path)\n",
    "#     rec = si.bandpass_filter(rec, dtype='float32')\n",
    "#     rec = si.common_reference(rec, reference='global', operator='median')\n",
    "#     rec = si.zscore(rec)\n",
    "    \n",
    "#     norm_chan_recording = rec.get_traces()\n",
    "    \n",
    "#     spatial_cov, temporal_cov = noise_whitener(norm_chan_recording, 121, 50)\n",
    "    \n",
    "#     np.save(os.path.join(save_path, '/spatial_cov.npy'), spatial_cov)\n",
    "#     np.save(os.path.join(save_path, '/temporal_cov.npy'), temporal_cov)\n",
    "    \n",
    "\n",
    "def extract_real(bin_fp, meta_fp, pid, t_window, use_labels=True):\n",
    "    one = ONE()\n",
    "    ba = AllenAtlas()\n",
    "\n",
    "    sl = SpikeSortingLoader(pid=pid, one=one, atlas=ba)\n",
    "    spikes, clusters, channels = sl.load_spike_sorting()\n",
    "    clusters = sl.merge_clusters(spikes, clusters, channels)\n",
    "    \n",
    "    geom = read_geom_from_meta(Path(meta_fp))\n",
    "    spike_times = spikes['times']\n",
    "    spike_frames = sl.samples2times(spike_times, direction='reverse').astype('int')\n",
    "    spike_train = np.concatenate((spike_frames.copy()[:,None], spikes['clusters'].copy()[:,None]), axis=1)\n",
    "    in_rec_idxs = np.where((spike_frames >= t_window[0]*30000) & (spike_frames <= t_window[1]*30000))[0]\n",
    "    spike_train = spike_train[in_rec_idxs, :]\n",
    "    spike_train[:, 0] = spike_train[:, 0] - t_window[0]*30000\n",
    "\n",
    "    channel_index = make_contiguous_channel_index(geom.shape[0], n_neighbors=40)\n",
    "    closest_channel_list = []\n",
    "    for cluster_id in spikes['clusters']:\n",
    "        closest_channel = clusters['channels'][cluster_id]\n",
    "        closest_channel_list.append(closest_channel)\n",
    "    closest_channels = np.asarray(closest_channel_list)\n",
    "    closest_channels = closest_channels[in_rec_idxs]\n",
    "    \n",
    "    aligned_spike_train, order, templates, template_shifts = spike_train_utils.clean_align_and_get_templates(spike_train, geom.shape[0], bin_fp)\n",
    "    templates, _ = snr_templates.get_templates(aligned_spike_train, geom, bin_fp, closest_channels, reducer=np.median)\n",
    "    mcs = np.array([templates[unit_id].ptp(0).argmax(0) for unit_id in range(len(templates))])\n",
    "    \n",
    "    spike_index = aligned_spike_train\n",
    "    if use_labels:\n",
    "        spike_index = np.vstack([spike_index[:, 0], np.array([mcs[unit] for unit in aligned_spike_train[:, 1]]), spike_index[:, 1]])\n",
    "    else:\n",
    "        spike_index[:, 1] = np.array([mcs[unit] for unit in aligned_spike_train[:, 1]])\n",
    "    \n",
    "    return spike_index, geom, channel_index, templates\n",
    "    \n",
    "    \n",
    "def extract_sim(rec_path, wfs_per_unit, use_labels=True):\n",
    "    recgen = mr.load_recordings(rec_path, load_waveforms=False)\n",
    "    # recgen.extract_templates(cut_out=[1.9,1.91], recompute=True)\n",
    "    geom_original = recgen.channel_positions[()]\n",
    "    depth_order = np.argsort(geom_original[:,2])\n",
    "    geom = geom_original[depth_order]\n",
    "    sort = si.MEArecSortingExtractor(rec_path)\n",
    "    \n",
    "    rec = si.MEArecRecordingExtractor(rec_path)\n",
    "    rec = si.bandpass_filter(rec, dtype='float32')\n",
    "    rec = si.common_reference(rec, reference='global', operator='median')\n",
    "    rec = si.zscore(rec)\n",
    "    \n",
    "    pre_peak = 42\n",
    "    post_peak = 79\n",
    "\n",
    "    folder = 'waveform_folder'\n",
    "    we = si.extract_waveforms(\n",
    "        rec,\n",
    "        sort,\n",
    "        folder,\n",
    "        ms_before=(1/rec.sampling_frequency)*pre_peak*1000,\n",
    "        ms_after=(1/rec.sampling_frequency)*post_peak*1000,\n",
    "        max_spikes_per_unit=wfs_per_unit,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    templates = we.get_all_templates()\n",
    "    templates = templates[:, :, depth_order]\n",
    "    \n",
    "#     print(we)\n",
    "    mcs = np.array([np.argmax(template.ptp(0)) for template in templates])\n",
    "#     mcs_template = np.array([templates[unit_id].ptp(0).argsort()[::-1][:4] for unit_id in len(templates)])\n",
    "#     shifts = np.abs(waveforms[:,:,mcs_template]).max(1).argmax(1)\n",
    "#     mcs_shifted = mcs_template[shifts]\n",
    "    \n",
    "    spike_train = sort.get_all_spike_trains()[0][0]\n",
    "    spike_units = np.array([int(unit[1:]) for unit in sort.get_all_spike_trains()[0][1]])\n",
    "    spike_unit_mcs = np.array([mcs[unit] for unit in spike_units])\n",
    "    spike_index = np.vstack([spike_train, spike_units, spike_unit_mcs]) if use_labels else np.vstack([spike_train, spike_unit_mcs])\n",
    "    \n",
    "    return spike_index.T, geom, we\n",
    "\n",
    "\n",
    "def make_dataset(bin_path, spike_index, geom, save_folder, we=None, templates=None, chan_index=None, num_chans_extract=21, unit_ids=None, train_num=1200, val_num=100, test_num=200, plot=False):\n",
    "    num_waveforms = train_num + val_num + test_num\n",
    "    spikes_array = []\n",
    "    geom_locs_array = []\n",
    "    max_chan_array = []\n",
    "    num_template_amps_shift = 4\n",
    "    num_chans = math.floor(num_chans_extract/2)\n",
    "    tot_num_chans = geom.shape[0]\n",
    "    if we is not None:\n",
    "        depth_order = np.argsort(geom[:,2])\n",
    "        geom = geom[depth_order]\n",
    "    num_waveforms = train_num + val_num + test_num\n",
    "\n",
    "    SMALL_SIZE = 14\n",
    "    MEDIUM_SIZE = 18\n",
    "    BIGGER_SIZE = 22\n",
    "    plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(12 + int(num_chans_extract*0.5), 24))\n",
    "        gs = GridSpec(len(unit_ids), num_chans_extract+1, figure=fig)\n",
    "        x = np.arange(121)\n",
    "    \n",
    "    if unit_ids is None:\n",
    "#         if we is not None:\n",
    "#             waveforms = we.get_waveforms(\"#{}\".format(str(unit_id)))\n",
    "#             waveforms = waveforms[:, :, depth_order]    # print(waveforms.shape)\n",
    "#         else:\n",
    "        waveforms, _ = read_waveforms(spike_index[:, 0], bin_file, \n",
    "                                      n_channels=geom.shape[0], spike_length_samples=121)\n",
    "        mcs = spike_index[:, 1]\n",
    "        for i, waveform in enumerate(waveforms):\n",
    "            mc_curr = mcs[i]\n",
    "            mc_start = max(mc_curr - num_chans, 0)\n",
    "            mc_end = min(mc_curr + num_chans, tot_num_chans) + 1\n",
    "            crop_wf, crop_chan, crop_geom, mask = pad_channels(waveform.T[mc_start:mc_end], \n",
    "                                                               geom[mc_start:mc_end, 1:], mc_start, mc_end, \n",
    "                                                               num_chans_extract)\n",
    "            spikes_array.append(crop_wf)\n",
    "            geom_locs_array.append(crop_geom)\n",
    "        max_chan_array = mcs\n",
    "        \n",
    "    else:\n",
    "        for k, unit_id in enumerate(unit_ids):\n",
    "            curr_temp_wfs = []\n",
    "            curr_geom_locs = []\n",
    "            curr_spike_max_chan = []\n",
    "            \n",
    "            if len(spike_index[:,0][np.where(spike_index[:,1]==unit_id)[0]]) < num_waveforms:\n",
    "                continue\n",
    "            if we is not None:\n",
    "                waveforms = we.get_waveforms(\"#{}\".format(str(unit_id)))\n",
    "                waveforms = waveforms[:num_waveforms, :, depth_order]\n",
    "                templates = we.get_all_templates()\n",
    "            else:\n",
    "                spike_frames_template = np.random.choice(spike_index[:,0][np.where(spike_index[:,2] == i)[0]], \n",
    "                                                         size=num_waveforms)\n",
    "                waveforms, _ = read_waveforms(spike_frames_template, bin_file, channel_index=channel_index,\n",
    "                                              n_channels=geom.shape[0], spike_length_samples=121)\n",
    "            mc = np.unique(spike_index[:, 2][np.where(spike_index[:,1] == unit_id)[0]])[0]\n",
    "#             print(np.unique(spike_index[:, 2][np.where(spike_index[:,1] == unit_id)[0]]))\n",
    "            mcs_template = templates[unit_id].ptp(0).argsort()[::-1][:num_template_amps_shift]\n",
    "            shifts = np.abs(waveforms[:,:,mcs_template]).max(1).argmax(1)\n",
    "            mcs_shifted = mcs_template[shifts]\n",
    "            for i, waveform in enumerate(waveforms):\n",
    "                mc_curr = mcs_shifted[i]\n",
    "                mc_start = max(mc_curr - num_chans, 0)\n",
    "                mc_end = min(mc_curr + num_chans + 1, tot_num_chans)\n",
    "                crop_wf, crop_chan, crop_geom, mask = pad_channels(waveform.T[mc_start:mc_end],\n",
    "                                                                   geom[mc_start:mc_end, 1:], mc_start, mc_end, \n",
    "                                                                   num_chans_extract)\n",
    "                curr_temp_wfs.append(crop_wf)\n",
    "                curr_geom_locs.append(crop_geom)\n",
    "                curr_spike_max_chan.append(crop_chan)\n",
    "\n",
    "            curr_temp_wfs = np.asarray(curr_temp_wfs)\n",
    "            curr_geom_locs = np.asarray(curr_geom_locs)\n",
    "            print(curr_temp_wfs.shape)\n",
    "\n",
    "            spikes_array.append(curr_temp_wfs)\n",
    "            geom_locs_array.append(curr_geom_locs)\n",
    "            max_chan_array.append(curr_spike_max_chan)\n",
    "\n",
    "            if plot:\n",
    "                ax0 = fig.add_subplot(gs[k, 0])\n",
    "                ax0.title.set_text('Unit {}'.format(str(unit_id)))\n",
    "                ax0.plot(x, templates[unit_id, :, mc])\n",
    "                ax0.axes.get_xaxis().set_visible(False)\n",
    "                ax1 = fig.add_subplot(gs[k, 1:], sharey=ax0)\n",
    "                for waveform in curr_temp_wfs[:100]:\n",
    "                    ax1.plot(waveform.flatten(), color='blue', alpha=.1)\n",
    "                    ax1.plot(templates[unit_id].T[mc_start:mc_end].flatten(), color='red')\n",
    "                    ax1.axes.get_yaxis().set_visible(False)\n",
    "                    ax1.axes.get_xaxis().set_visible(False)\n",
    "\n",
    "        fig.subplots_adjust(wspace=0, hspace=0.25)\n",
    "    \n",
    "    spikes_array = np.array(spikes_array)\n",
    "    geom_locs_array = np.array(geom_locs_array)\n",
    "    max_chan_array = np.array(max_chan_array)\n",
    "    print(spikes_array.shape)\n",
    "    print(geom_locs_array.shape)\n",
    "    print(max_chan_array.shape)\n",
    "    np.save(os.path.join(save_path, 'full_raw_spikes.npy'), spikes_array)\n",
    "    np.save(os.path.join(save_path, 'channel_spike_locs.npy'), geom_locs_array)\n",
    "            \n",
    "    train_set, val_set, test_set = split_data(spikes_array, train_num, val_num, test_num, num_chans_extract)\n",
    "    train_geom_locs, val_geom_locs, test_geom_locs = split_data(geom_locs_array, train_num, val_num, test_num, num_chans_extract)\n",
    "    train_max_chan, val_max_chan, test_max_chan = split_data(max_chan_array, train_num, val_num, test_num, num_chans_extract)\n",
    "            \n",
    "    np.save(os.path.join(save_path, 'spikes_train.npy'), train_set)\n",
    "    np.save(os.path.join(save_path, 'spikes_val.npy'), val_set)\n",
    "    np.save(os.path.join(save_path, 'spikes_test.npy'), test_set)\n",
    "            \n",
    "    np.save(os.path.join(save_path, 'channel_spike_locs_train.npy'), train_geom_locs)\n",
    "    np.save(os.path.join(save_path, 'channel_spike_locs_val.npy'), val_geom_locs)\n",
    "    np.save(os.path.join(save_path, 'channel_spike_locs_test.npy'), test_geom_locs)\n",
    "    \n",
    "    np.save(os.path.join(save_path, 'channel_num_train.npy'), train_max_chan)\n",
    "    np.save(os.path.join(save_path, 'channel_num_val.npy'), val_max_chan)\n",
    "    np.save(os.path.join(save_path, 'channel_num_test.npy'), test_max_chan)\n",
    "    \n",
    "    np.save(os.path.join(save_path, 'geom.npy'), geom)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/ankit/Documents/PaninskiLab/contrastive_spikes/mearec_v2'\n",
    "sim_bin_path = '/Users/ankit/Documents/PaninskiLab/spike-psvae/notebook/Neuropixels-64_static_uniform_homogeneous-001.h5'\n",
    "sim_spike_idx, sim_geom, sim_we = extract_sim(sim_bin_path, 1500)\n",
    "selected_sim_units = [2, 5, 7, 13, 16, 21, 29, 43, 47, 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset(sim_bin_path, sim_spike_idx, sim_geom, save_path, sim_we, unit_ids=selected_sim_units, num_chans_extract=1, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/ankit/Documents/PaninskiLab/contrastive_spikes/dytest'\n",
    "real_bin_path = '/Users/ankit/Documents/PaninskiLab/spike-psvae/notebook/destriped__spikeglx_ephysData_g0_t0.imec0.ap.stream.cbin'\n",
    "meta_file = '/Users/ankit/Documents/PaninskiLab/spike-psvae/notebook/destriped__spikeglx_ephysData_g0_t0.imec0.ap.stream.meta'\n",
    "pid = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "real_spike_idx, real_geom, chan_idx, temps = extract_real(real_bin_path, meta_file, pid, [200, 500])\n",
    "selected_real_units = [9, 11, 13, 65, 79, 82, 267, 258, 318, 329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset(real_bin_path, real_spike_idx, real_geom, save_path, chan_index=chan_idx, templates=temps, unit_ids=selected_real_units, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_spike_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b379a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/ankit/Documents/PaninskiLab/contrastive_spikes/dytest'\n",
    "real_bin_path = '/Users/ankit/Documents/PaninskiLab/spike-psvae/notebook/destriped__spikeglx_ephysData_g0_t0.imec0.ap.stream.cbin'\n",
    "meta_file = '/Users/ankit/Documents/PaninskiLab/spike-psvae/notebook/destriped__spikeglx_ephysData_g0_t0.imec0.ap.stream.meta'\n",
    "pid = 'febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "real_spike_idx, real_geom, chan_idx, temps = extract_real(real_bin_path, meta_file, pid, [0, 600])\n",
    "selected_real_units = [9, 11, 13, 65, 79, 82, 267, 258, 318, 329]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
